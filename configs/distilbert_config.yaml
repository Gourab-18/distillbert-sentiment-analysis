# DistilBERT Training Configuration
# This file contains all hyperparameters for training DistilBERT on sentiment analysis

# Model Configuration
model:
  name: "distilbert-base-uncased"
  num_labels: 2
  dropout: 0.1
  attention_dropout: 0.1

# Data Configuration
data:
  train_size: 20000
  val_size: 5000
  test_size: 25000
  max_length: 512
  train_batch_size: 16
  eval_batch_size: 32
  num_workers: 2

# Training Configuration
training:
  num_epochs: 3
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
  fp16: false  # Set to true if using GPU with mixed precision

# Optimizer Configuration
optimizer:
  type: "AdamW"
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Scheduler Configuration
scheduler:
  type: "linear"  # linear warmup and decay
  warmup_ratio: 0.1

# Evaluation Configuration
evaluation:
  eval_steps: 500  # Evaluate every N steps
  save_steps: 500  # Save checkpoint every N steps
  logging_steps: 100  # Log metrics every N steps
  save_total_limit: 2  # Keep only best 2 checkpoints
  metric_for_best_model: "eval_accuracy"
  greater_is_better: true
  load_best_model_at_end: true

# Weights & Biases Configuration
wandb:
  enabled: true
  project: "distilbert-sentiment-analysis"
  entity: null  # Set your W&B username/team or leave null for default
  run_name: null  # Auto-generated if null
  tags: ["distilbert", "sentiment-analysis", "imdb"]
  notes: "Fine-tuning DistilBERT for binary sentiment classification on IMDb reviews"

# Paths Configuration
paths:
  data_dir: "../data/processed/tokenized_distilbert-base-uncased"
  output_dir: "../models/distilbert-sentiment"
  logs_dir: "../logs"
  cache_dir: "../cache"

# Reproducibility
seed: 42

# Device Configuration
device:
  auto_detect: true  # Automatically detect GPU/CPU
  gpu_id: 0  # GPU ID if multiple GPUs available
