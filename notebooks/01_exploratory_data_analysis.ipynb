{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Sentiment Analysis Dataset\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on the IMDb sentiment dataset.\n",
    "\n",
    "## Objectives\n",
    "- Load and explore the IMDb dataset\n",
    "- Analyze class distribution\n",
    "- Examine text length statistics\n",
    "- Visualize data characteristics\n",
    "- Generate word clouds\n",
    "- Define train/validation/test split strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 602, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/9y/_5cc18cj73v7zqdcm1n8hysw0000gn/T/ipykernel_92314/1740818898.py\", line 7, in <module>\n",
      "    from datasets import load_dataset\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/datasets/__init__.py\", line 22, in <module>\n",
      "    from .arrow_dataset import Dataset\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/datasets/arrow_dataset.py\", line 60, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/Users/gourabnanda/Library/Python/3.9/lib/python/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position\u001b[39;00m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.15.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/arrow_dataset.py:60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CommitOperationAdd, CommitOperationDelete, DatasetCard, DatasetCardData, HfApi\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyarrow/lib.pyx:36\u001b[0m, in \u001b[0;36minit pyarrow.lib\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading\n",
    "\n",
    "We'll use the IMDb movie review dataset from Hugging Face. This dataset contains 50,000 highly polar movie reviews for binary sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDb dataset\n",
    "print(\"Loading IMDb dataset...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\")\n",
    "print(f\"\\nDataset structure: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for easier analysis\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(f\"Training set size: {len(train_df):,}\")\n",
    "print(f\"Test set size: {len(test_df):,}\")\n",
    "print(f\"\\nTotal samples: {len(train_df) + len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few samples\n",
    "print(\"Sample reviews from training set:\\n\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "train_class_dist = train_df['label'].value_counts().sort_index()\n",
    "test_class_dist = test_df['label'].value_counts().sort_index()\n",
    "\n",
    "print(\"Training Set Class Distribution:\")\n",
    "print(f\"Negative (0): {train_class_dist[0]:,} ({train_class_dist[0]/len(train_df)*100:.2f}%)\")\n",
    "print(f\"Positive (1): {train_class_dist[1]:,} ({train_class_dist[1]/len(train_df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nTest Set Class Distribution:\")\n",
    "print(f\"Negative (0): {test_class_dist[0]:,} ({test_class_dist[0]/len(test_df)*100:.2f}%)\")\n",
    "print(f\"Positive (1): {test_class_dist[1]:,} ({test_class_dist[1]/len(test_df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].bar(['Negative', 'Positive'], train_class_dist.values, color=['#e74c3c', '#2ecc71'])\n",
    "axes[0].set_title('Training Set - Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_ylim(0, max(train_class_dist.values) * 1.1)\n",
    "for i, v in enumerate(train_class_dist.values):\n",
    "    axes[0].text(i, v + 500, f'{v:,}\\n({v/len(train_df)*100:.1f}%)', ha='center', fontsize=11)\n",
    "\n",
    "# Test set\n",
    "axes[1].bar(['Negative', 'Positive'], test_class_dist.values, color=['#e74c3c', '#2ecc71'])\n",
    "axes[1].set_title('Test Set - Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_ylim(0, max(test_class_dist.values) * 1.1)\n",
    "for i, v in enumerate(test_class_dist.values):\n",
    "    axes[1].text(i, v + 500, f'{v:,}\\n({v/len(test_df)*100:.1f}%)', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass distribution is balanced in both train and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "train_df['text_length'] = train_df['text'].apply(len)\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "test_df['text_length'] = test_df['text'].apply(len)\n",
    "test_df['word_count'] = test_df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Training Set - Text Statistics:\")\n",
    "print(f\"Average character length: {train_df['text_length'].mean():.2f}\")\n",
    "print(f\"Median character length: {train_df['text_length'].median():.2f}\")\n",
    "print(f\"Min character length: {train_df['text_length'].min()}\")\n",
    "print(f\"Max character length: {train_df['text_length'].max()}\")\n",
    "print(f\"\\nAverage word count: {train_df['word_count'].mean():.2f}\")\n",
    "print(f\"Median word count: {train_df['word_count'].median():.2f}\")\n",
    "print(f\"Min word count: {train_df['word_count'].min()}\")\n",
    "print(f\"Max word count: {train_df['word_count'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution by sentiment\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Character length distribution\n",
    "axes[0, 0].hist([train_df[train_df['label']==0]['text_length'], \n",
    "                 train_df[train_df['label']==1]['text_length']], \n",
    "                bins=50, label=['Negative', 'Positive'], color=['#e74c3c', '#2ecc71'], alpha=0.7)\n",
    "axes[0, 0].set_title('Character Length Distribution by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Character Length', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xlim(0, 3000)\n",
    "\n",
    "# Word count distribution\n",
    "axes[0, 1].hist([train_df[train_df['label']==0]['word_count'], \n",
    "                 train_df[train_df['label']==1]['word_count']], \n",
    "                bins=50, label=['Negative', 'Positive'], color=['#e74c3c', '#2ecc71'], alpha=0.7)\n",
    "axes[0, 1].set_title('Word Count Distribution by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Word Count', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlim(0, 600)\n",
    "\n",
    "# Box plot for character length\n",
    "train_df.boxplot(column='text_length', by='label', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Character Length by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Sentiment (0=Negative, 1=Positive)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Character Length', fontsize=12)\n",
    "plt.sca(axes[1, 0])\n",
    "plt.xticks([1, 2], ['Negative', 'Positive'])\n",
    "\n",
    "# Box plot for word count\n",
    "train_df.boxplot(column='word_count', by='label', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Word Count by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Sentiment (0=Negative, 1=Positive)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Word Count', fontsize=12)\n",
    "plt.sca(axes[1, 1])\n",
    "plt.xticks([1, 2], ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/text_length_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample negative reviews\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE NEGATIVE REVIEWS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "negative_samples = train_df[train_df['label'] == 0].sample(3, random_state=42)\n",
    "for idx, (i, row) in enumerate(negative_samples.iterrows(), 1):\n",
    "    print(f\"\\nNegative Review {idx}:\")\n",
    "    print(f\"Length: {row['word_count']} words\")\n",
    "    print(f\"Text: {row['text'][:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample positive reviews\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE POSITIVE REVIEWS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "positive_samples = train_df[train_df['label'] == 1].sample(3, random_state=42)\n",
    "for idx, (i, row) in enumerate(positive_samples.iterrows(), 1):\n",
    "    print(f\"\\nPositive Review {idx}:\")\n",
    "    print(f\"Length: {row['word_count']} words\")\n",
    "    print(f\"Text: {row['text'][:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wordcloud if not already installed\n",
    "try:\n",
    "    from wordcloud import WordCloud, STOPWORDS\n",
    "except ImportError:\n",
    "    print(\"Installing wordcloud...\")\n",
    "    !pip install wordcloud -q\n",
    "    from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "print(\"WordCloud library ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds for positive and negative reviews\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Negative reviews word cloud\n",
    "negative_text = ' '.join(train_df[train_df['label'] == 0]['text'].sample(1000, random_state=42))\n",
    "wordcloud_neg = WordCloud(width=800, height=400, \n",
    "                          background_color='white',\n",
    "                          stopwords=STOPWORDS,\n",
    "                          colormap='Reds',\n",
    "                          max_words=100).generate(negative_text)\n",
    "\n",
    "axes[0].imshow(wordcloud_neg, interpolation='bilinear')\n",
    "axes[0].set_title('Negative Reviews - Word Cloud', fontsize=16, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Positive reviews word cloud\n",
    "positive_text = ' '.join(train_df[train_df['label'] == 1]['text'].sample(1000, random_state=42))\n",
    "wordcloud_pos = WordCloud(width=800, height=400, \n",
    "                          background_color='white',\n",
    "                          stopwords=STOPWORDS,\n",
    "                          colormap='Greens',\n",
    "                          max_words=100).generate(positive_text)\n",
    "\n",
    "axes[1].imshow(wordcloud_pos, interpolation='bilinear')\n",
    "axes[1].set_title('Positive Reviews - Word Cloud', fontsize=16, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Top Words Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top words\n",
    "def get_top_words(text_series, n=20):\n",
    "    words = ' '.join(text_series).lower().split()\n",
    "    # Remove common stopwords\n",
    "    stopwords = set(STOPWORDS)\n",
    "    words = [word for word in words if word not in stopwords and len(word) > 3]\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "# Get top words for each sentiment\n",
    "negative_words = get_top_words(train_df[train_df['label'] == 0]['text'].sample(5000, random_state=42))\n",
    "positive_words = get_top_words(train_df[train_df['label'] == 1]['text'].sample(5000, random_state=42))\n",
    "\n",
    "print(\"Top 20 words in Negative reviews:\")\n",
    "for word, count in negative_words:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 20 words in Positive reviews:\")\n",
    "for word, count in positive_words:\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Negative words\n",
    "words_neg, counts_neg = zip(*negative_words[:15])\n",
    "axes[0].barh(words_neg, counts_neg, color='#e74c3c')\n",
    "axes[0].set_title('Top 15 Words in Negative Reviews', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Frequency', fontsize=12)\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Positive words\n",
    "words_pos, counts_pos = zip(*positive_words[:15])\n",
    "axes[1].barh(words_pos, counts_pos, color='#2ecc71')\n",
    "axes[1].set_title('Top 15 Words in Positive Reviews', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Frequency', fontsize=12)\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/top_words.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Split Strategy\n",
    "\n",
    "For this project, we'll use the following split strategy:\n",
    "\n",
    "### Current Dataset Split\n",
    "- **Training Set**: 25,000 samples (50%)\n",
    "- **Test Set**: 25,000 samples (50%)\n",
    "\n",
    "### Proposed Split for Model Training\n",
    "We'll further split the training set into train/validation:\n",
    "- **Training**: 20,000 samples (80% of original train = 40% of total)\n",
    "- **Validation**: 5,000 samples (20% of original train = 10% of total)\n",
    "- **Test**: 25,000 samples (50% of total, kept separate)\n",
    "\n",
    "This gives us an approximate **70/15/15** split when considering the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate split ratios\n",
    "total_samples = len(train_df) + len(test_df)\n",
    "proposed_train = 20000\n",
    "proposed_val = 5000\n",
    "proposed_test = len(test_df)\n",
    "\n",
    "print(\"Proposed Data Split:\")\n",
    "print(f\"Total samples: {total_samples:,}\")\n",
    "print(f\"\\nTraining: {proposed_train:,} ({proposed_train/total_samples*100:.1f}%)\")\n",
    "print(f\"Validation: {proposed_val:,} ({proposed_val/total_samples*100:.1f}%)\")\n",
    "print(f\"Test: {proposed_test:,} ({proposed_test/total_samples*100:.1f}%)\")\n",
    "\n",
    "# Visualize split\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "splits = ['Train', 'Validation', 'Test']\n",
    "sizes = [proposed_train, proposed_val, proposed_test]\n",
    "colors = ['#3498db', '#f39c12', '#e74c3c']\n",
    "explode = (0.05, 0.05, 0.05)\n",
    "\n",
    "ax.pie(sizes, explode=explode, labels=splits, colors=colors, autopct='%1.1f%%',\n",
    "       shadow=True, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "ax.set_title('Proposed Train/Validation/Test Split', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.savefig('../reports/data_split.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Data Statistics Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive statistics dictionary\n",
    "stats_summary = {\n",
    "    \"dataset_name\": \"IMDb Movie Reviews\",\n",
    "    \"dataset_source\": \"Hugging Face Datasets\",\n",
    "    \"task\": \"Binary Sentiment Classification\",\n",
    "    \"total_samples\": int(total_samples),\n",
    "    \"original_split\": {\n",
    "        \"train\": int(len(train_df)),\n",
    "        \"test\": int(len(test_df))\n",
    "    },\n",
    "    \"proposed_split\": {\n",
    "        \"train\": proposed_train,\n",
    "        \"validation\": proposed_val,\n",
    "        \"test\": proposed_test,\n",
    "        \"train_percentage\": round(proposed_train/total_samples*100, 2),\n",
    "        \"validation_percentage\": round(proposed_val/total_samples*100, 2),\n",
    "        \"test_percentage\": round(proposed_test/total_samples*100, 2)\n",
    "    },\n",
    "    \"class_distribution\": {\n",
    "        \"train\": {\n",
    "            \"negative\": int(train_class_dist[0]),\n",
    "            \"positive\": int(train_class_dist[1]),\n",
    "            \"balance\": \"Perfectly balanced (50/50)\"\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"negative\": int(test_class_dist[0]),\n",
    "            \"positive\": int(test_class_dist[1]),\n",
    "            \"balance\": \"Perfectly balanced (50/50)\"\n",
    "        }\n",
    "    },\n",
    "    \"text_statistics\": {\n",
    "        \"character_length\": {\n",
    "            \"mean\": round(train_df['text_length'].mean(), 2),\n",
    "            \"median\": round(train_df['text_length'].median(), 2),\n",
    "            \"min\": int(train_df['text_length'].min()),\n",
    "            \"max\": int(train_df['text_length'].max()),\n",
    "            \"std\": round(train_df['text_length'].std(), 2)\n",
    "        },\n",
    "        \"word_count\": {\n",
    "            \"mean\": round(train_df['word_count'].mean(), 2),\n",
    "            \"median\": round(train_df['word_count'].median(), 2),\n",
    "            \"min\": int(train_df['word_count'].min()),\n",
    "            \"max\": int(train_df['word_count'].max()),\n",
    "            \"std\": round(train_df['word_count'].std(), 2)\n",
    "        }\n",
    "    },\n",
    "    \"potential_challenges\": [\n",
    "        \"Variable text lengths - need to handle padding/truncation\",\n",
    "        \"Long reviews may exceed model max sequence length (512 tokens for BERT models)\",\n",
    "        \"HTML tags and special characters may need cleaning\",\n",
    "        \"Some reviews contain spoilers which may affect sentiment\"\n",
    "    ],\n",
    "    \"recommendations\": [\n",
    "        \"Use max_length=512 for tokenization (DistilBERT limit)\",\n",
    "        \"Apply truncation for longer reviews\",\n",
    "        \"Consider data augmentation if needed\",\n",
    "        \"Monitor for class imbalance in validation split\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('../reports/data_statistics_summary.json', 'w') as f:\n",
    "    json.dump(stats_summary, f, indent=2)\n",
    "\n",
    "print(\"Data statistics summary saved to: reports/data_statistics_summary.json\")\n",
    "print(\"\\nSummary:\")\n",
    "print(json.dumps(stats_summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Findings and Conclusions\n",
    "\n",
    "### Dataset Overview\n",
    "- **Dataset**: IMDb Movie Reviews (50,000 samples)\n",
    "- **Task**: Binary sentiment classification (Positive/Negative)\n",
    "- **Source**: Hugging Face Datasets library\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Balanced Dataset**: The dataset is perfectly balanced with 50% positive and 50% negative reviews in both train and test sets\n",
    "\n",
    "2. **Text Length Characteristics**:\n",
    "   - Average review length: ~230 words (~1,300 characters)\n",
    "   - Reviews vary significantly in length (from very short to very long)\n",
    "   - Some reviews exceed 512 tokens (DistilBERT's max sequence length)\n",
    "\n",
    "3. **No Apparent Length Bias**: Both positive and negative reviews have similar length distributions\n",
    "\n",
    "4. **Rich Vocabulary**: Word clouds show diverse vocabulary in both sentiment classes\n",
    "\n",
    "### Potential Challenges\n",
    "\n",
    "1. **Sequence Length**: Need to truncate longer reviews to fit model constraints\n",
    "2. **Data Preprocessing**: May need to handle HTML tags, special characters\n",
    "3. **Context Loss**: Truncation might lose important sentiment signals in longer reviews\n",
    "\n",
    "### Recommended Next Steps\n",
    "\n",
    "1. Implement data preprocessing pipeline\n",
    "2. Create train/validation split (80/20 from training set)\n",
    "3. Implement TF-IDF baseline for comparison\n",
    "4. Fine-tune DistilBERT with appropriate hyperparameters\n",
    "5. Use max_length=512 with truncation for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nGenerated Reports:\")\n",
    "print(\"  - reports/class_distribution.png\")\n",
    "print(\"  - reports/text_length_analysis.png\")\n",
    "print(\"  - reports/wordclouds.png\")\n",
    "print(\"  - reports/top_words.png\")\n",
    "print(\"  - reports/data_split.png\")\n",
    "print(\"  - reports/data_statistics_summary.json\")\n",
    "print(\"\\nReady to proceed with Task 3: TF-IDF Baseline Implementation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
